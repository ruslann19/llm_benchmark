\newpage

\section{Обзор существующих бенчмарков}

Современные подходы к оценке больших языковых моделей можно разделить на две основные категории: статические и динамические бенчмарки. В данном разделе рассматриваются ключевые представители каждого типа, их методология, сильные стороны и ограничения.

\subsection*{MMLU (Massive Multitask Language Understanding)}

MMLU \cite{hendrycks2021measuringmassivemultitasklanguage} является одним из наиболее распространённых статических бенчмарков. Он состоит из 15 908 вопросов с множественным выбором, охватывающих 57 академических дисциплин -- от математики и права до питания и религии. Формат тестирования предполагает выбор одного правильного ответа из четырёх предложенных, что позволяет автоматически вычислять точность. Бенчмарк был создан в 2020 году и быстро стал стандартом для сравнения моделей благодаря своей широте и сложности.

Однако MMLU страдает от фундаментальных проблем статичности: вопросы фиксированы и со временем попадают в тренировочные данные новых LLM, что приводит к «загрязнению» (contamination) и искусственному завышению результатов. Несмотря на это, MMLU продолжает широко упоминаться в исследованиях и сравнительных таблицах. Это связано с его исторической ролью как первого масштабного многозадачного теста, а также с тем, что он предоставляет простую и сопоставимую метрику, удобную для отслеживания общего прогресса в отрасли.

Для преодоления ограничений MMLU был предложен его улучшенный вариант -- \textbf{MMLU-Pro}. Он расширяет набор ответов с четырёх до десяти вариантов, устраняет тривиальные и шумные вопросы из оригинального набора и делает акцент на сложных рассуждениях, а не только на фактологических знаниях. Это приводит к значительному падению точности моделей (на 16–33%) и повышает устойчивость результатов к вариациям в формулировках промптов. MMLU-Pro остаётся статичным набором, но его повышенная сложность временно отодвигает проблему насыщаемости.

\begin{table}[h]
    \centering
    \caption{Примеры вопросов из различных предметных областей MMLU}
    \begin{tabular}{|p{0.9\textwidth}|}
        \hline
        \textbf{Question 1 (Abstract Algebra):}                                                  \\
        Find all \( c \) in \(\mathbb{Z}_3\) such that \(\mathbb{Z}_3[x]/(x^2 + c)\) is a field. \\
        \begin{itemize}
            \item[(A)] 0
            \item[\textbf{(B)}] \textbf{1}
            \item[(C)] 2
            \item[(D)] 3
        \end{itemize}                                                            \\
        \hline
    \end{tabular}
\end{table}

\newpage

\begin{table}[h]
    \centering
    \begin{tabular}{|p{0.9\textwidth}|}
        \hline
        \textbf{Question 2 (International Law):}                                                                                                                              \\
        Would a reservation to the definition of torture in the \textbf{International Covenant on Civil and Political Rights (ICCPR)} be acceptable in contemporary practice? \\
        \begin{itemize}
            \item[(A)] This is an acceptable reservation if the reserving country's legislation employs a different definition.
            \item[\textbf{(B)}] \textbf{This is an unacceptable reservation because it contravenes the object and purpose of the ICCPR.}
            \item[(C)] This is an unacceptable reservation because the definition of torture in the ICCPR is consistent with customary international law.
            \item[(D)] This is an acceptable reservation because under general international law States have the right to enter reservations to treaties.
        \end{itemize}                          \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[!h]
    \centering
    \begin{tabular}{|p{0.9\textwidth}|}
        \hline
        \textbf{Question 3 (Professional Medicine):}                                                                                                                                                                                                                                                                                                                                                                                        \\
        A 33-year-old man undergoes a radical thyroidectomy for thyroid cancer. During the operation, moderate hemorrhaging requires ligation of several vessels in the left side of the neck. Postoperatively, serum studies show a calcium concentration of 7.5 mg/dL, albumin concentration of 4 g/dL, and parathyroid hormone concentration of 200 pg/mL. Damage to which of the following vessels caused the findings in this patient? \\
        \begin{itemize}
            \item[(A)] Branch of the costocervical trunk.
            \item[(B)] Branch of the external carotid artery.
            \item[\textbf{(C)}] \textbf{Branch of the thyrocervical trunk.}
            \item[(D)] Tributary of the internal jugular vein.
        \end{itemize}                                                                                                                                                                                                                                                                                                                                                                      \\
        \hline
    \end{tabular}
\end{table}

\subsection*{Humanity's Last Exam (HLE)}

Humanity's Last Exam \cite{phan2025humanitysexam} -- это статический бенчмарк, созданный как ответ на стремительный прогресс LLM. Его цель -- предложить вопросы, находящиеся на переднем крае человеческих знаний. Набор состоит из более чем 2500 экспертных вопросов, охватывающих математику, физику, биологию, гуманитарные науки и другие области. Около 24% вопросов являются множественным выбором, остальные требуют краткого ответа.

Ключевая особенность HLE -- строгий процесс контроля качества. Вопросы создаются и проверяются экспертами с учёными степенями (PhD), а за лучшие вопросы авторы получают денежные призы. Такой подход обеспечивает высокую сложность и надёжность вопросов, но делает бенчмарк крайне затратным и медленным в обновлении. Кроме того, в 2025 году независимое исследование выявило потенциальные ошибки в части ответов, что подчёркивает трудности поддержания безупречного качества в таких масштабных экспертных проектах. HLE служит «высокой планкой» для самых передовых моделей, но его статичность и медленный цикл обновления не позволяют ему стать инструментом для оперативного сравнения.

\begin{table}[!h]
    \centering
    \caption{Пример вопроса из Humanity's Last Exam}
    \begin{tabular}{|p{0.9\textwidth}|}
        \hline
        \textbf{Question:}                                                                                                                                                                                                                                                                          \\
        Hummingbirds within Apodiformes uniquely have a bilaterally paired oval bone, a sesamoid embedded in the caudolateral portion of the expanded, cruciate aponeurosis of insertion of m. depressor caudae. How many paired tendons are supported by this sesamoid bone? Answer with a number. \\
        \hline
    \end{tabular}
\end{table}

\subsection*{Chatbot Arena}
Chatbot Arena \cite{chiang2024chatbotarenaopenplatform} -- это краудсорсинговый бенчмарк для оценки языковых моделей, основанный на прямых сравнениях.
Пользователи заходят на веб-платформу и вводят любой интересующий их запрос.
Система анонимно выбирает две разные языковые модели и генерирует их ответы на этот запрос.
Пользователю показываются два ответа без указания, какая модель их сгенерировала.
Задача пользователя -- сравнить ответы и выбрать лучший, либо отметить ничью.
Каждое такое голосование называется «баттлом».
На основе миллионов подобных парных сравнений вычисляется рейтинг для каждой модели.
Для расчёта рейтинга используется система Эло, аналогичная шахматной.
Чем чаще модель выигрывает в сравнениях, тем выше её позиция на публичном лидерборде.
Таким образом, лидерборд отражает коллективные человеческие предпочтения, а не автоматические метрики.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/chatbot_arena_interface.png}
    \caption{Интерфейс оценивания в Chatbot Arena}
\end{figure}

\subsubsection*{Плюсы Chatbot Arena}
Ключевым преимуществом Chatbot Arena является динамическое обновление лидерборда.
В отличие от статических тестов, рейтинг здесь «живой» и меняется в реальном времени по мере поступления новых голосов.
Это позволяет мгновенно отражать выход новых версий моделей или обновлений.
Система устойчива к целевой оптимизации или «переобучению» под себя.
Разработчики не могут намеренно «подстроить» модель под конкретные вопросы бенчмарка, так как набор запросов непредсказуем.
Модели в каждом баттле выбираются анонимно, поэтому невозможно подготовиться к соревнованию с конкретным оппонентом.
Критерий оценки -- человеческое предпочтение -- сложно имитировать с помощью автоматических методов.
Сам массив запросов крайне разнообразен и создаётся реальными пользователями, что делает его практически невоспроизводимым для тренировки.
Это обеспечивает высокую объективность оценки способности модели общаться в реальных условиях.

\subsubsection*{Минусы Chatbot Arena}
Главным минусом является предвзятость пользовательской аудитории, формирующей лидерборд.
Активнее всего голосуют технические специалисты, разработчики и энтузиасты ИИ.
Это приводит к тематическому смещению в запросах и оценках.
В бенчмарке непропорционально много вопросов по программированию, машинному обучению и технологиям.
В то же время сильно недопредставлены бытовые, творческие или узкопрофессиональные темы (например, медицина или право).
Стилевые предпочтения этой аудитории также влияют на результат: часто ценится излишняя детализация и технический жаргон, что не всегда оптимально для среднего пользователя.
Существует и асимметрия в данных: популярные коммерческие модели участвуют в большем числе баттлов, что позволяет точнее оценить их рейтинг.
Модели с открытыми весами часто оказываются в менее выгодном положении из-за меньшего количества сравнений.
Наконец, сохраняется теоретический риск манипуляций с голосованием, хотя команда разработчиков активно внедряет защитные меры, такие как CAPTCHA и системы обнаружения аномалий.

\subsection*{LiveBench}
LiveBench \cite{white2025livebenchchallengingcontaminationlimitedllm} -- это открытый автоматизированный бенчмарк для оценки больших языковых моделей, созданный для решения проблем устаревания и «загрязнения» тестовых данных.
Его ключевая идея -- использование часто обновляемых вопросов, основанных на недавно опубликованных данных.
Вопросы генерируются из актуальных источников: свежих научных статей на arXiv, новостей, датасетов, описаний фильмов и задач с математических олимпиад.
Каждый вопрос имеет объективный, проверяемый эталонный ответ (ground truth), что позволяет оценивать модели автоматически, без привлечения людей или других ИИ-судей.
Бенчмарк охватывает шесть основных категорий: математика, программирование, логические рассуждения, понимание языка, следование инструкциям и анализ данных.
Внутри каждой категории есть несколько специализированных задач, всего более 18 задач.
Для минимизации риска, что модели уже встречали тестовые данные во время обучения, набор вопросов регулярно обновляется, а публикация самых свежих вопросов задерживается.
Оценка модели по LiveBench даёт единый процентный балл, а также детальную разбивку по категориям, что позволяет понять её сильные и слабые стороны.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/livebench_leaderboard.png}
    \caption{Вершина лидерборда LiveBench на момент 08.01.2026}
\end{figure}

\subsubsection*{Плюсы LiveBench}
Главным преимуществом LiveBench является его динамическая и устойчивая к загрязнению природа.
Регулярное обновление вопросов из актуальных источников делает бенчмарк «живым» и значительно усложняет целевое переобучение моделей под него.
Поскольку модели не могли заранее видеть эти конкретные вопросы в своих тренировочных данных, их результаты точнее отражают реальные способности к рассуждению и решению задач, а не навык запоминания.
Объективная проверка по эталонным ответам исключает субъективные предубеждения, которые присущи оценке с помощью LLM-судей или краудсорсинга.
Например, известно, что LLM-судьи часто предпочитают более многословные ответы или ответы, сгенерированные моделями, схожими с ними самими.
LiveBench избегает этой ловушки, обеспечивая полностью автоматическую и беспристрастную оценку.
Широта охвата -- от решения математических задач до анализа табличных данных -- делает бенчмарк комплексным инструментом для оценки общих возможностей модели.
Сложность задач подобрана так, что даже лучшие модели на момент создания показывали accuracy ниже 70\%, что позволяет эффективно различать современные мощные модели.

\subsubsection*{Минусы LiveBench}
Основным и наиболее критичным для русскоязычного контекста минусом является языковая ограниченность бенчмарка.
LiveBench создан и существует исключительно на английском языке.
Все вопросы, инструкции и эталонные ответы представлены на английском.
Бенчмарк напрямую не оценивает и не учитывает способности модели понимать, генерировать или рассуждать на русском языке или любом другом языке, кроме английского.
Это делает его результаты малорелевантными для оценки применимости модели в русскоязычной среде, будь то бизнес-задачи, поддержка клиентов или создание контента.
Несмотря на заявленную сложность, некоторые эксперты отмечают, что по мере роста популярности любого бенчмарка, включая LiveBench, возникает риск его «взлома» -- целевой оптимизации моделей для достижения высоких результатов на конкретных типах задач бенчмарка, что может не коррелировать с общим качеством.
Кроме того, несмотря на широкий охват категорий, бенчмарк всё же может иметь перекос в сторону формальных, структурированных задач (математика, код, головоломки) в ущерб оценке креативности, социального интеллекта или сложного многоступенчатого диалога.
Наконец, хотя автоматическая оценка объективна, она может быть менее гибкой для задач, где возможны несколько правильных или частично правильных ответов, которые сложно свести к одному эталону.

\subsection*{Бенчмарки специального назначения}

Также можно выделить узкоспециализированные бенчмарки, которые проверяют отдельные способности модели.

\begin{itemize}
    \item Например, игровой бенчмарк LLMArena \cite{chen2024llmarenaassessingcapabilitieslarge} тестирует модель в 6 разных игровых средах: покер, аукционы, командные игры и др.
          Этот бенчмарк позволяет выяснить, отдельные аспекты моделей: пространственное-мышление, способность оценивать риск, математические способности, способность работать в команде и др.

    \item Бенчмарк PingPong \cite{gusev2025pingpongbenchmarkroleplayinglanguage}, проверяющий, насколько LLM способна поддерживать ролевое общение в диалоге, насколько правдоподобно это получается у модели.

    \item Бенчмарк RuQualBench \cite{kristaller} оценивает, насколько модель знает грамматику русского языка.
          Но он тестирует исключительно грамматику, а не общие знания.
\end{itemize}

Поскольку в рамках данной работы стоит задача разработки бенчмарка для оценивания общих знаний, то не будем подробно вдаваться в детали данных бенчмарков, а лишь обойдёмся их упоминанием.
Полезно понимать, что общие знания - не единственный аспект, который можно измерять у LLM.
