\newpage

\section{Эксперименты}

\subsection*{Автоматизированное тестирование DeepSeek через API}

Для автоматизированного тестирования была использована модель DeepSeek, доступная по API.
Тестирование проводилось на последовательных версиях бенчмарка (v1-v4).
Результаты, полученные автоматически с использованием текущей версии системы LLM-as-a-Judge, представлены в Таблице~\ref{tab:deepseek-api-results}.

\begin{table}[h]
    \centering
    \caption{Результаты автоматизированного тестирования DeepSeek на разных версиях бенчмарка}
    \label{tab:deepseek-api-results}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Версия бенчмарка} & \textbf{Модель} & \textbf{Accuracy} & \textbf{OK} & \textbf{FAIL} \\
        \hline
        1                         & DeepSeek        & 0.27              & 27          & 73            \\
        \hline
        2                         & DeepSeek        & 0.42              & 42          & 58            \\
        \hline
        3                         & DeepSeek        & 0.36              & 36          & 64            \\
        \hline
        4                         & DeepSeek        & 0.34              & 34          & 66            \\
        \hline
    \end{tabular}
\end{table}

\textbf{Анализ результатов:}
Как видно из таблицы, метрика accuracy, рассчитанная автоматическим судьёй, варьируется в диапазоне от 0.27 до 0.42.
Значительный разброс показателей между версиями бенчмарка указывает на возможную нестабильность либо самого судьи, либо на существенные различия в сложности вопросов между срезами данных.
Среднее значение accuracy \(\approx 0.35\) является невысоким, что требует дополнительного исследования.
Качественный анализ ошибочных ответов показал, что \textbf{LLM-as-a-Judge в текущей конфигурации демонстрирует излишнюю строгость}, засчитывая как некорректные ответы, которые являются семантически верными, но не совпадают с эталоном дословно (например, синонимичные формулировки или развернутые объяснения вместо краткого факта).

\subsection*{Ручное тестирование моделей через веб-интерфейс}

Для получения предварительной сравнительной картины различных моделей был проведён эксперимент с использованием веб-интерфейсов.
В данном случае использовался \textbf{альтернативный метод оценки}: ответы нескольких моделей на одну и ту же подборку вопросов были получены через веб-интерфейс и поданы \textbf{единым пакетом} в LLM-судью (DeepSeek) через её веб-интерфейс.
Промпт судьи содержал инструкцию сравнить все ответы с эталоном сразу в рамках таблицы.
Результаты данной пакетной оценки представлены в Таблице~\ref{tab:batch-web-results}.

\begin{table}[!h]
    \centering
    \caption{Сравнительные результаты ручного тестирования моделей через веб-интерфейс (оценка эксперта)}
    \label{tab:batch-web-results}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Ранг} & \textbf{Модель} & \textbf{Accuracy} & \textbf{OK} & \textbf{FAIL} \\
        \hline
        1             & DeepSeek        & 0.717             & 167         & 66            \\
        \hline
        2             & Qwen            & 0.614             & 143         & 90            \\
        \hline
        3             & YandexGPT       & 0.575             & 129         & 95            \\
        \hline
        4             & GigaChat        & 0.442             & 103         & 130           \\
        \hline
        5             & Perplexity      & 0.365             & 85          & 148           \\
        \hline
    \end{tabular}
\end{table}
\textbf{Анализ:}
Пакетная оценка показала ожидаемую оценку качества среди моделей, при этом абсолютные значения метрик (от 0.365 до 0.717) оказались значительно выше, чем в эксперименте с индивидуальными API-запросами.
Это позволяет сделать два важных вывода:
\begin{enumerate}
    \item \textbf{Консистентность ранжирования:} LLM-судья успешно выполняет сравнительную функцию, выстраивая модели в порядке, соответствующем их ожидаемой мощности (Модель A > B > C > D > E).
    \item \textbf{Влияние метода оценки:} Существенное расхождение в абсолютных значениях accuracy (пакетная оценка дала результаты в \(\approx 2\) раза выше) указывает на критическую зависимость итоговой метрики от \textbf{методологии взаимодействия с судьёй}. Различия в промптах (объём инструкций, наличие few-shot примеров, формат ввода данных) и режиме обработки (пакетный vs. поштучный) напрямую влияют на строгость суждения.
\end{enumerate}

\textbf{План корректировок и дальнейших работ:}
\begin{enumerate}
    \item \textbf{Стандартизация и калибровка LLM-as-a-Judge:} Для перехода к полностью автоматизированному API-тестированию необходимо разработать и зафиксировать эталонный промпт и протокол взаимодействия, которые обеспечат баланс между строгостью и смысловой адекватностью оценок. За основу будет взят подход, показавший более реалистичные результаты при пакетной оценке.
    \item \textbf{Валидация против человеческой оценки:} Ключевым шагом станет создание «золотого» датасета размеченных человеком ответов для калибровки автоматического судьи и последующего контроля его работы.
    \item \textbf{Переход на массовое API-тестирование:} После калибровки судьи система будет доработана для проведения массовых тестов всех целевых моделей исключительно через их официальные API. Это обеспечит воспроизводимость, объективность сравнения и возможность регулярного обновления лидерборда.
\end{enumerate}

Таким образом, проведённые эксперименты в большей степени выявляют и количественно определяют методологическую неопределённость в системе автоматической оценки, нежели измеряют абсолютные способности моделей.
Устранение этой неопределённости составляет основную инженерную задачу следующего этапа работы.
